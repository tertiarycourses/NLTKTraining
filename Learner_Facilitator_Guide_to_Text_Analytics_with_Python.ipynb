{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Learner/Facilitator Guide to Text Analytics with Python\n"
      ],
      "metadata": {
        "id": "U85KzbhZ1DDq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('all')"
      ],
      "metadata": {
        "id": "dnpQrsI9FKZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Topic 2 Text Cleaning and Pre-processing\n"
      ],
      "metadata": {
        "id": "LO-UPm--2dIF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read Text using Standard Python Package\n",
        "\n",
        "Python supports a number of standard and custom libraries to read files of all types into python variables."
      ],
      "metadata": {
        "id": "osiyI4G05ppg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pumaqEhw054Y"
      },
      "outputs": [],
      "source": [
        "#Read the file using standard python libaries\n",
        "with open(\"Spark-Course-Description.txt\", 'r') as fh:  \n",
        "    filedata = fh.read()\n",
        "    \n",
        "#Print first 200 characters in the file\n",
        "print(\"Data read from file : \", filedata[0:200] )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read Text using NLTK CorpusReader\n",
        "\n",
        "Read the same text file using a Corpus Reader\n",
        "\n",
        "NLTK supports multiple CorpusReaders depending upon the type of data source. Details available in http://www.nltk.org/howto/corpus.html"
      ],
      "metadata": {
        "id": "5AAycGQm5lYK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#install nltk from anaconda prompt using \"pip install nltk\"\n",
        "import nltk\n",
        "#Download punkt package, used part of the other commands\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
        "\n",
        "#Read the file into a corpus. The same command can read an entire directory\n",
        "corpus=PlaintextCorpusReader(\"\",\"Spark-Course-Description.txt\")\n",
        "\n",
        "#Print raw contents of the corpus\n",
        "print(corpus.raw())"
      ],
      "metadata": {
        "id": "lcpd--Xt1Isv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explore the Corpus"
      ],
      "metadata": {
        "id": "EMW5tm7s7rnm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Extract the file IDs from the corpus\n",
        "print(\"Files in this corpus : \", corpus.fileids())\n",
        "\n",
        "#Extract paragraphs from the corpus\n",
        "paragraphs=corpus.paras()\n",
        "print(\"\\n Total paragraphs in this corpus : \", len(paragraphs))\n",
        "\n",
        "#Extract sentences from the corpus\n",
        "sentences=corpus.sents()\n",
        "print(\"\\n Total sentences in this corpus : \", len(sentences))\n",
        "print(\"\\n The first sentence : \", sentences[0])\n",
        "\n",
        "#Extract words from the corpus\n",
        "print(\"\\n Words in this corpus : \",corpus.words() )\n"
      ],
      "metadata": {
        "id": "qd0yLeRT50tO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analyze the Corpus\n",
        "\n",
        "The NLTK library provides a number of functions to analyze the distributions and aggregates for data in the corpus.\n"
      ],
      "metadata": {
        "id": "1sSq0NU48Ebo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Find the frequency distribution of words in the corpus\n",
        "course_freq_dist=nltk.FreqDist(corpus.words())\n",
        "\n",
        "#Print most commonly used words\n",
        "print(\"Top 10 words in the corpus : \", course_freq_dist.most_common(10))\n",
        "\n",
        "#find the distribution for a specific word\n",
        "print(\"\\n Distribution for \\\"Spark\\\" : \",course_freq_dist.get(\"Spark\"))"
      ],
      "metadata": {
        "id": "1t8Ns-rP7DX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenization\n",
        "\n",
        "Tokenization refers to converting a text string into individual tokens. Tokens may be words or punctations"
      ],
      "metadata": {
        "id": "QGXIMrjn9WiP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import os\n",
        "\n",
        "\n",
        "#Read the base file into a raw text variable\n",
        "base_file = open(\"Spark-Course-Description.txt\", 'rt')\n",
        "raw_text = base_file.read()\n",
        "base_file.close()\n",
        "\n",
        "#Extract tokens\n",
        "token_list = nltk.word_tokenize(raw_text)\n",
        "print(\"Token List : \",token_list[:20])\n",
        "print(\"\\n Total Tokens : \",len(token_list))"
      ],
      "metadata": {
        "id": "qizAEN3U700N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cleansing Text\n",
        "\n",
        "We will see examples of removing punctuation and converting to lower case\n",
        "\n",
        "#### Remove Punctuation"
      ],
      "metadata": {
        "id": "EXKUpbBw-gw2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Use the Punkt library to extract tokens\n",
        "token_list2 = list(filter(lambda token: nltk.tokenize.punkt.PunktToken(token).is_non_punct, token_list))\n",
        "print(\"Token List after removing punctuation : \",token_list2[:20])\n",
        "print(\"\\nTotal tokens after removing punctuation : \", len(token_list2))"
      ],
      "metadata": {
        "id": "8NGII_GX9jpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Convert to Lower Case"
      ],
      "metadata": {
        "id": "-JpYjfuWAZKG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token_list3=[word.lower() for word in token_list2 ]\n",
        "print(\"Token list after converting to lower case : \", token_list3[:20])\n",
        "print(\"\\nTotal tokens after converting to lower case : \", len(token_list3))"
      ],
      "metadata": {
        "id": "9jMN98Em-t6S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stop word Removal\n",
        "\n",
        "Removing stop words by using a standard stop word list available in NLTK for English"
      ],
      "metadata": {
        "id": "pOvK1fuzBMSc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Download the standard stopword list\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "#Remove stopwords\n",
        "token_list4 = list(filter(lambda token: token not in stopwords.words('english'), token_list3))\n",
        "print(\"Token list after removing stop words : \", token_list4[:20])\n",
        "print(\"\\nTotal tokens after removing stop words : \", len(token_list4))"
      ],
      "metadata": {
        "id": "M0a7d8vIAbuu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stemming"
      ],
      "metadata": {
        "id": "srAVe2EJCC0_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Use the PorterStemmer library for stemming.\n",
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "#Stem data\n",
        "token_list5 = [stemmer.stem(word) for word in token_list4 ]\n",
        "print(\"Token list after stemming : \", token_list5[:20])\n",
        "print(\"\\nTotal tokens after Stemming : \", len(token_list5))"
      ],
      "metadata": {
        "id": "utEgWr4MBQFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lemmatization"
      ],
      "metadata": {
        "id": "z37ZE6pRCH9i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Use the wordnet library to map words to their lemmatized form\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "token_list6 = [lemmatizer.lemmatize(word) for word in token_list4 ]\n",
        "print(\"Token list after Lemmatization : \", token_list6[:20])\n",
        "print(\"\\nTotal tokens after Lemmatization : \", len(token_list6))"
      ],
      "metadata": {
        "id": "4pvYRUXQCFx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Comparison of tokens between raw, stemming and lemmatization"
      ],
      "metadata": {
        "id": "XWO9gyhdJQE2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Check for token technlogies\n",
        "print( \"Raw : \", token_list4[20],\" , Stemmed : \", token_list5[20], \" , Lemmatized : \", token_list6[20])"
      ],
      "metadata": {
        "id": "3BWQp_nDCK_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### N-gram\n"
      ],
      "metadata": {
        "id": "BVJb3W5IJnkZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Prepare data for use in this exercise\n",
        "\n",
        "import nltk\n",
        "import os\n",
        "#Download punkt package, used part of the other commands\n",
        "nltk.download('punkt')\n",
        "\n",
        "#Read the base file into a token list\n",
        "base_file = open(\"Spark-Course-Description.txt\", 'rt')\n",
        "raw_text = base_file.read()\n",
        "base_file.close()\n",
        "\n",
        "#Execute the same pre-processing done in module 3\n",
        "token_list = nltk.word_tokenize(raw_text)\n",
        "\n",
        "token_list2 = list(filter(lambda token: nltk.tokenize.punkt.PunktToken(token).is_non_punct, token_list))\n",
        "\n",
        "token_list3=[word.lower() for word in token_list2 ]\n",
        "\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "token_list4 = list(filter(lambda token: token not in stopwords.words('english'), token_list3))\n",
        "\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "token_list6 = [lemmatizer.lemmatize(word) for word in token_list4 ]\n",
        "\n",
        "print(\"\\n Total Tokens : \",len(token_list6))"
      ],
      "metadata": {
        "id": "_aBr-ON0JR5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.util import ngrams\n",
        "from collections import Counter\n",
        "\n",
        "#Find bigrams and print the most common 5\n",
        "bigrams = ngrams(token_list6,2)\n",
        "print(\"Most common bigrams : \")\n",
        "print(Counter(bigrams).most_common(5))\n",
        "\n",
        "#Find trigrams and print the most common 5\n",
        "trigrams = ngrams(token_list6,3)\n",
        "print(\" \\n Most common trigrams : \" )\n",
        "print(Counter(trigrams).most_common(5))"
      ],
      "metadata": {
        "id": "jx7GZd72JrCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TF-IDF"
      ],
      "metadata": {
        "id": "sRLDhR2WYKGo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Use scikit-learn library\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "#Use a small corpus for each visualization\n",
        "vector_corpus = [\n",
        "    'NBA is a Basketball league',\n",
        "    'Basketball is popular in America.',\n",
        "    'TV in America telecast BasketBall.',\n",
        "]\n",
        "\n",
        "#Create a vectorizer for english language\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "\n",
        "#Create the vector\n",
        "tfidf=vectorizer.fit_transform(vector_corpus)\n",
        "\n",
        "print(\"Tokens used as features are : \")\n",
        "print(vectorizer.get_feature_names())\n",
        "\n",
        "print(\"\\n Size of array. Each row represents a document. Each column represents a feature/token\")\n",
        "print(tfidf.shape)\n",
        "\n",
        "print(\"\\n Actual TF-IDF array\")\n",
        "tfidf.toarray()"
      ],
      "metadata": {
        "id": "U6LH3BCsLtF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Topic 3 Text Analytics\n"
      ],
      "metadata": {
        "id": "VjYOtdTDK1LH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parts-of-Speech (POS) Tagging\n",
        "\n",
        "Some examples of Parts-of-Speech abbreviations:\n",
        "NN : noun\n",
        "NNS : noun plural\n",
        "VBP : Verb singular present."
      ],
      "metadata": {
        "id": "BizdTXjLK6P-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#download the tagger package\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "#Tag and print the first 10 tokens\n",
        "nltk.pos_tag(token_list4)[:10]"
      ],
      "metadata": {
        "id": "iAzhNL78KBWR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Name Entity Recognition (NER)\n"
      ],
      "metadata": {
        "id": "VTLKmSr7POXC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        " \n",
        "# Step Two: Load Data\n",
        " \n",
        "sentence = \"WASHINGTON -- In the wake of a string of abuses by New York police officers in the 1990s, Loretta E. Lynch, the top federal prosecutor in Brooklyn, spoke forcefully about the pain of a broken trust that African-Americans felt and said the responsibility for repairing generations of miscommunication and mistrust fell to law enforcement.\"\n",
        "\n",
        "# Step Three: Tokenise, find parts of speech and chunk words \n",
        "\n",
        "for sent in nltk.sent_tokenize(sentence):\n",
        "  for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
        "     if hasattr(chunk, 'label'):\n",
        "        print(chunk.label(), ' '.join(c[0] for c in chunk))"
      ],
      "metadata": {
        "id": "msdSsK5BPKNr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text Clustering"
      ],
      "metadata": {
        "id": "FioGvx2CVmrU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Preparing Text for Clustering"
      ],
      "metadata": {
        "id": "PQ9-h_R4V53j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "#Load course hashtags\n",
        "hashtags_df=pd.read_csv(\"Course-Hashtags.csv\")\n",
        "print(\"\\nSample hashtag data :\")\n",
        "print(hashtags_df[:2])\n",
        "\n",
        "#Seperate Hashtags and titles to lists\n",
        "hash_list = hashtags_df[\"HashTags\"].tolist()\n",
        "title_list = hashtags_df[\"Course\"].tolist()\n",
        "\n",
        "#Do TF-IDF conversion of hashtags\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "hash_matrix=vectorizer.fit_transform(hash_list)\n",
        "print(\"\\n Feature names Identified :\\n\")\n",
        "print(vectorizer.get_feature_names())"
      ],
      "metadata": {
        "id": "yCiQ-Lo7Q5Md"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Clustering TF-IDF data"
      ],
      "metadata": {
        "id": "c6_al5OjWCFx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Use KMeans clustering from scikit-learn\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "#Split data into 3 clusters\n",
        "kmeans = KMeans(n_clusters=3).fit(hash_matrix)\n",
        "\n",
        "#get Cluster labels.\n",
        "clusters=kmeans.labels_\n",
        "\n",
        "#Print cluster label and Courses under each cluster\n",
        "for group in set(clusters):\n",
        "    print(\"\\nGroup : \",group, \"\\n-------------------\")\n",
        "    \n",
        "    for i in hashtags_df.index:\n",
        "        if ( clusters[i] == group):\n",
        "            print(title_list[i])\n",
        "    "
      ],
      "metadata": {
        "id": "W0oh0beAV015"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Finding optimal Cluster size"
      ],
      "metadata": {
        "id": "K0qWAzlnWIIX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Find optimal cluster size by finding sum-of-squared-distances\n",
        "\n",
        "sosd = []\n",
        "#Run clustering for sizes 1 to 15 and capture inertia\n",
        "K = range(1,15)\n",
        "for k in K:\n",
        "    km = KMeans(n_clusters=k)\n",
        "    km = km.fit(hash_matrix)\n",
        "    sosd.append(km.inertia_)\n",
        "    \n",
        "print(\"Sum of squared distances : \" ,sosd)\n",
        "\n",
        "#Plot sosd against number of clusters\n",
        "import matplotlib.pyplot as mpLib\n",
        "mpLib.plot(K, sosd, 'bx-')\n",
        "mpLib.xlabel('Cluster count')\n",
        "mpLib.ylabel('Sum_of_squared_distances')\n",
        "mpLib.title('Elbow Method For Optimal Cluster Size')\n",
        "mpLib.show()"
      ],
      "metadata": {
        "id": "r2XmwuQwV5Ap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Topic 4 Sentimental Analysis"
      ],
      "metadata": {
        "id": "vX-skmiBZQjv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sentiment Analysis using TextBlob"
      ],
      "metadata": {
        "id": "LmOocyQVbHuL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Preparing Data for Sentiment Analysis"
      ],
      "metadata": {
        "id": "FY6M5DuW4Iwm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import the movie reviews corpus\n",
        "with open(\"Movie-Reviews.txt\", 'r') as fh:  \n",
        "    reviews = fh.readlines()\n",
        "print(reviews[:2])"
      ],
      "metadata": {
        "id": "mZkhnIcSWLgX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Finding Sentiments by Review"
      ],
      "metadata": {
        "id": "COjeJCMQ4PVt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#install textblob if not already installed using \"pip install -U textblob\"\n",
        "from textblob import TextBlob\n",
        "\n",
        "print('{:40} : {:10} : {:10}'.format(\"Review\", \"Polarity\", \"Subjectivity\") )\n",
        "\n",
        "for review in reviews:\n",
        "    #Find sentiment of a review\n",
        "    sentiment = TextBlob(review)\n",
        "    #Print individual sentiments\n",
        "    print('{:40} :   {: 01.2f}    :   {:01.2f}'.format(review[:40]\\\n",
        "                , sentiment.polarity, sentiment.subjectivity) )"
      ],
      "metadata": {
        "id": "bBJJb17f4F28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Summarizing Sentiment"
      ],
      "metadata": {
        "id": "9qzz3yFk4WnC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Categorize Polarity into Positive, Neutral or Negative\n",
        "labels = [\"Negative\", \"Neutral\", \"Positive\"]\n",
        "#Initialize count array\n",
        "values =[0,0,0]\n",
        "\n",
        "#Categorize each review\n",
        "for review in reviews:\n",
        "    sentiment = TextBlob(review)\n",
        "    \n",
        "    #Custom formula to convert polarity \n",
        "    # 0 = (Negative) 1 = (Neutral) 2=(Positive)\n",
        "    polarity = round(( sentiment.polarity + 1 ) * 3 ) % 3\n",
        "    \n",
        "    #add the summary array\n",
        "    values[polarity] = values[polarity] + 1\n",
        "    \n",
        "print(\"Final summarized counts :\", values)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "#Set colors by label\n",
        "colors=[\"Green\",\"Blue\",\"Red\"]\n",
        "\n",
        "print(\"\\n Pie Representation \\n-------------------\")\n",
        "#Plot a pie chart\n",
        "plt.pie(values, labels=labels, colors=colors, \\\n",
        "        autopct='%1.1f%%', shadow=True, startangle=140)\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "C2WuifUM4S3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text Classificatoin"
      ],
      "metadata": {
        "id": "Zk8hnIzXQboD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Preparing Data for Classification"
      ],
      "metadata": {
        "id": "cz8pGl7f_r0b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Read course descriptions\n",
        "with open(\"Course-Descriptions.txt\", 'r') as fh:  \n",
        "    descriptions = fh.read().splitlines()\n",
        "print(\"Sample course descriptions :\", descriptions[:2])\n",
        "\n",
        "#Setup stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "#setup wordnet for lemmatization\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "#Custom tokenizer that will perform tokenization, stopword removal\n",
        "#and lemmatization\n",
        "def customtokenize(str):\n",
        "    tokens=nltk.word_tokenize(str)\n",
        "    nostop = list(filter(lambda token: token not in stopwords.words('english'), tokens))\n",
        "    lemmatized=[lemmatizer.lemmatize(word) for word in nostop ]\n",
        "    return lemmatized\n",
        "\n",
        "#Generate TFIDF matrix\n",
        "vectorizer = TfidfVectorizer(tokenizer=customtokenize)\n",
        "tfidf=vectorizer.fit_transform(descriptions)\n",
        "\n",
        "print(\"\\nSample feature names identified : \", vectorizer.get_feature_names()[:25])\n",
        "print(\"\\nSize of TFIDF matrix : \",tfidf.shape)\n"
      ],
      "metadata": {
        "id": "EGrgcg6O4aWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Building and Training the model"
      ],
      "metadata": {
        "id": "z4ZuOUCf_t1E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading the pre-built classifications for training\n",
        "with open(\"Course-Classification.txt\", 'r') as fh:  \n",
        "    classifications = fh.read().splitlines()\n",
        "\n",
        "#Create Labels and integer classes\n",
        "from sklearn import preprocessing\n",
        "\n",
        "le = preprocessing.LabelEncoder()\n",
        "le.fit(classifications)\n",
        "print(\"Classes found : \", le.classes_)\n",
        "\n",
        "#Convert classes to integers for use with ML\n",
        "int_classes = le.transform(classifications)\n",
        "print(\"\\nClasses converted to integers :\", int_classes)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "#Split as training and testing sets\n",
        "xtrain, xtest, ytrain, ytest = train_test_split(tfidf, int_classes,random_state=0)\n",
        "\n",
        "#Build the model\n",
        "classifier= MultinomialNB().fit(xtrain, ytrain)\n"
      ],
      "metadata": {
        "id": "yMc-QNd8_mvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model Evaluation"
      ],
      "metadata": {
        "id": "21gLUd5J_9Yl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "print(\"Testing with Test Data :\\n------------------------\")\n",
        "#Predict on test data\n",
        "predictions=classifier.predict(xtest)\n",
        "print(\"Confusion Matrix : \")\n",
        "print(metrics.confusion_matrix(ytest, predictions))\n",
        "print(\"\\n Prediction Accuracy : \",  \\\n",
        "      metrics.accuracy_score(ytest, predictions) )\n",
        "\n",
        "print(\"\\nTesting with Full Corpus :\\n--------------------------\")\n",
        "#Predict on entire corpus data\n",
        "predictions=classifier.predict(tfidf)\n",
        "print(\"Confusion Matrix : \")\n",
        "print(metrics.confusion_matrix(int_classes, predictions))\n",
        "print(\"\\n Prediction Accuracy : \",  \\\n",
        "      metrics.accuracy_score(int_classes, predictions) )\n"
      ],
      "metadata": {
        "id": "Fl9hqwNw_zON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Topic 5 Text Summarization"
      ],
      "metadata": {
        "id": "QQVIRwvs9ZFa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text summarization using sumy"
      ],
      "metadata": {
        "id": "ybNnXe_gKdcD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "aCeAAPdDDHgo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = \"Paris is the capital and most populous city of France, with an estimated population of 2,175,601 residents as of 2018, in an area of more than 105 square kilometres (41 square miles). The City of Paris is the centre and seat of government of the region and province of Île-de-France, or Paris Region, which has an estimated population of 12,174,880, or about 18 percent of the population of France as of 2017.\""
      ],
      "metadata": {
        "id": "h_2ju4ovNS88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "from pprint import pprint\n",
        "\n",
        "classifier = pipeline(\"summarization\")\n",
        "summary_text = classifier(sample_text)\n",
        "pprint(summary_text)"
      ],
      "metadata": {
        "id": "5eKgXMmrM6pk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text summarization using transformer BART Model"
      ],
      "metadata": {
        "id": "tV-9BOXFKhwL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = \"\"\"There is a massive shift underway in the global economy. Fueled by a global pandemic, we are seeing economic, cultural and social trends colliding.\n",
        "\n",
        "This has led to a rapid transformation in how we work, where we work and even why we work. The only constant is change. And change that previously took decades, has happened in just two years. At Linkedin, we call this “The Great Reshuffle”.\n",
        "\n",
        "Running in parallel with “The Great Reshuffle” is the world’s urgent need to save itself. Sir David Attenborough tells us “the future of humanity and indeed, all life on earth, now depends on us”.\n",
        "\n",
        "Our hope is that we can collectively turn the climate crisis into an opportunity for change. We can all help save the planet by creating and pursuing greener careers for the world’s workforce. This green transition can see existing jobs apply more green skills and new green jobs emerge in tandem.\n",
        "\n",
        "Such jobs include Sustainability Manager, Wind Turbine Technician, Solar Consultant, Ecologist, Environmental Health & Safety Specialist: roles that barely existed just a decade ago yet today are the five fastest-growing green jobs globally.\n",
        "\n",
        "So what does this mean for you? How can you get yourself ready for the green economy that lies ahead?\n",
        "\n",
        "In my LinkedIn Learning course, Closing the Green Skills Gap to Power a Greener Economy, I’ll help you better understand the green transition and the opportunities it presents. You’ll learn more about green skills and understand the rise of green jobs. I’ll look at what different countries and sectors are doing to lead green change. I’ll also give practical advice about what you can do to embrace and take advantage of this rapid economic shift. Watch this course for free until May 19, 2022. \n",
        "\n",
        "Remember - change starts with you! So whether you’re taking the first steps in your career or looking for a new challenge in your existing one, here are three things you can do to turbocharge your career in the green economy.\n",
        "\n",
        "Upskill for green skills and green careers\n",
        "Improving your green skills is important to pursue emerging career opportunities. Globally, members with green skills have been more resilient to economic downturns than the rest of the workforce. So improving or adding to your green skills is a great place to start. You could consider self-directed learning, offline or online. You could also enroll in a green-related higher education course to help you upskill.\n",
        "\n",
        "Meanwhile, in the workplace, you could brush up on green skills as part of your organization's learning curriculum. Why not ask your employer or organization what green skills training may be available and even how you could volunteer to help make it happen?\n",
        "\n",
        "Nurture your network of green skilled workers\n",
        "Your network is something you can’t put a price on. That’s because you’ve built it by nurturing relationships and showcasing your capabilities. But you need to keep it up!\n",
        "\n",
        "Indeed our data shows that green skilled workers tend to have stronger networks, with two and sometimes three times more connections on average. Your network is the way to build new relationships, ignite conversations and find new opportunities.\n",
        "\n",
        "So start seeking out ‘green’ related content that interests you and consider engaging with it, for example, by commenting on it or sharing it. \n",
        "\n",
        "Why not join a LinkedIn Group dedicated to a green topic that you’re passionate about? And if you’re feeling more adventurous, you can even set up your own LinkedIn group, especially if you can’t find a green topic specific to your job.\n",
        "\n",
        "That way, you’ll ignite new conversations which could present new green opportunities for not only you but also your network.\"\"\""
      ],
      "metadata": {
        "id": "kWzisrXFDVTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bart_summarizer = pipeline(\"summarization\", model=\"facebook/bart-large\", tokenizer=\"facebook/bart-large\", framework=\"tf\")\n",
        "bart_summarized = bart_summarizer(sample_text, min_length=25, max_length=50,truncation=True)\n",
        "pprint(bart_summarized)"
      ],
      "metadata": {
        "id": "4PHzn2lKEg6f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}